{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41a83233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(r\"D:\\Desktop Codes Data\\Data Science\\Developement\\ipynb files\\Sales_Forcast_Project\\Developement\\Processing and Modeling\\data_set.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90eeee56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>asin</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>text</th>\n",
       "      <th>main_category</th>\n",
       "      <th>price</th>\n",
       "      <th>log_price</th>\n",
       "      <th>domain</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>sentiment_category</th>\n",
       "      <th>year</th>\n",
       "      <th>text_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B000000LWZ</td>\n",
       "      <td>B000000LWZ</td>\n",
       "      <td>5</td>\n",
       "      <td>1997-09-09 03:13:17</td>\n",
       "      <td>It was while driving on Highway 83, heading of...</td>\n",
       "      <td>Digital Music</td>\n",
       "      <td>10.98</td>\n",
       "      <td>2.396075</td>\n",
       "      <td>CDs_and_Vinyl</td>\n",
       "      <td>0.256641</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1997</td>\n",
       "      <td>1029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B000002H3Z</td>\n",
       "      <td>B000002H3Z</td>\n",
       "      <td>5</td>\n",
       "      <td>1997-09-13 22:05:50</td>\n",
       "      <td>This isn't actually a book. It is a beautifull...</td>\n",
       "      <td>Digital Music</td>\n",
       "      <td>14.98</td>\n",
       "      <td>2.706716</td>\n",
       "      <td>CDs_and_Vinyl</td>\n",
       "      <td>0.620000</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1997</td>\n",
       "      <td>337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B000003YSX</td>\n",
       "      <td>B000003YSX</td>\n",
       "      <td>5</td>\n",
       "      <td>1997-10-05 21:21:20</td>\n",
       "      <td>This album is not of what one would usually re...</td>\n",
       "      <td>Digital Music</td>\n",
       "      <td>15.99</td>\n",
       "      <td>2.771964</td>\n",
       "      <td>CDs_and_Vinyl</td>\n",
       "      <td>0.113333</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1997</td>\n",
       "      <td>754</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B000002H2H</td>\n",
       "      <td>B000002H2H</td>\n",
       "      <td>5</td>\n",
       "      <td>1997-10-20 14:48:57</td>\n",
       "      <td>Released on November 23 1984, Metallica's seco...</td>\n",
       "      <td>Digital Music</td>\n",
       "      <td>7.30</td>\n",
       "      <td>1.987874</td>\n",
       "      <td>CDs_and_Vinyl</td>\n",
       "      <td>0.149953</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>1997</td>\n",
       "      <td>1289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B000002AS3</td>\n",
       "      <td>B000002AS3</td>\n",
       "      <td>5</td>\n",
       "      <td>1997-10-21 17:42:32</td>\n",
       "      <td>This is the first cd made by Oasis. My favorit...</td>\n",
       "      <td>Digital Music</td>\n",
       "      <td>9.09</td>\n",
       "      <td>2.207175</td>\n",
       "      <td>CDs_and_Vinyl</td>\n",
       "      <td>0.417614</td>\n",
       "      <td>Positive</td>\n",
       "      <td>1997</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  parent_asin        asin  rating            timestamp  \\\n",
       "0  B000000LWZ  B000000LWZ       5  1997-09-09 03:13:17   \n",
       "1  B000002H3Z  B000002H3Z       5  1997-09-13 22:05:50   \n",
       "2  B000003YSX  B000003YSX       5  1997-10-05 21:21:20   \n",
       "3  B000002H2H  B000002H2H       5  1997-10-20 14:48:57   \n",
       "4  B000002AS3  B000002AS3       5  1997-10-21 17:42:32   \n",
       "\n",
       "                                                text  main_category  price  \\\n",
       "0  It was while driving on Highway 83, heading of...  Digital Music  10.98   \n",
       "1  This isn't actually a book. It is a beautifull...  Digital Music  14.98   \n",
       "2  This album is not of what one would usually re...  Digital Music  15.99   \n",
       "3  Released on November 23 1984, Metallica's seco...  Digital Music   7.30   \n",
       "4  This is the first cd made by Oasis. My favorit...  Digital Music   9.09   \n",
       "\n",
       "   log_price         domain  sentiment_score sentiment_category  year  \\\n",
       "0   2.396075  CDs_and_Vinyl         0.256641           Positive  1997   \n",
       "1   2.706716  CDs_and_Vinyl         0.620000           Positive  1997   \n",
       "2   2.771964  CDs_and_Vinyl         0.113333            Neutral  1997   \n",
       "3   1.987874  CDs_and_Vinyl         0.149953            Neutral  1997   \n",
       "4   2.207175  CDs_and_Vinyl         0.417614           Positive  1997   \n",
       "\n",
       "   text_length  \n",
       "0         1029  \n",
       "1          337  \n",
       "2          754  \n",
       "3         1289  \n",
       "4          179  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b26e08cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Loading data from data_set.csv...\n",
      "Dataset shape: (4095972, 13)\n",
      "\n",
      "Column types:\n",
      "parent_asin            object\n",
      "asin                   object\n",
      "rating                  int64\n",
      "timestamp              object\n",
      "text                   object\n",
      "main_category          object\n",
      "price                 float64\n",
      "log_price             float64\n",
      "domain                 object\n",
      "sentiment_score       float64\n",
      "sentiment_category     object\n",
      "year                    int64\n",
      "text_length             int64\n",
      "dtype: object\n",
      "\n",
      "Sample data:\n",
      "  parent_asin        asin  rating            timestamp  \\\n",
      "0  B000000LWZ  B000000LWZ       5  1997-09-09 03:13:17   \n",
      "1  B000002H3Z  B000002H3Z       5  1997-09-13 22:05:50   \n",
      "2  B000003YSX  B000003YSX       5  1997-10-05 21:21:20   \n",
      "3  B000002H2H  B000002H2H       5  1997-10-20 14:48:57   \n",
      "4  B000002AS3  B000002AS3       5  1997-10-21 17:42:32   \n",
      "\n",
      "                                                text  main_category  price  \\\n",
      "0  It was while driving on Highway 83, heading of...  Digital Music  10.98   \n",
      "1  This isn't actually a book. It is a beautifull...  Digital Music  14.98   \n",
      "2  This album is not of what one would usually re...  Digital Music  15.99   \n",
      "3  Released on November 23 1984, Metallica's seco...  Digital Music   7.30   \n",
      "4  This is the first cd made by Oasis. My favorit...  Digital Music   9.09   \n",
      "\n",
      "   log_price         domain  sentiment_score sentiment_category  year  \\\n",
      "0   2.396075  CDs_and_Vinyl         0.256641           Positive  1997   \n",
      "1   2.706716  CDs_and_Vinyl         0.620000           Positive  1997   \n",
      "2   2.771964  CDs_and_Vinyl         0.113333            Neutral  1997   \n",
      "3   1.987874  CDs_and_Vinyl         0.149953            Neutral  1997   \n",
      "4   2.207175  CDs_and_Vinyl         0.417614           Positive  1997   \n",
      "\n",
      "   text_length  \n",
      "0         1029  \n",
      "1          337  \n",
      "2          754  \n",
      "3         1289  \n",
      "4          179  \n",
      "\n",
      "Missing values per column:\n",
      "parent_asin           0\n",
      "asin                  0\n",
      "rating                0\n",
      "timestamp             0\n",
      "text                  0\n",
      "main_category         0\n",
      "price                 0\n",
      "log_price             0\n",
      "domain                0\n",
      "sentiment_score       0\n",
      "sentiment_category    0\n",
      "year                  0\n",
      "text_length           0\n",
      "dtype: int64\n",
      "Starting data preprocessing...\n",
      "Using categorical features: ['domain', 'main_category', 'sentiment_category']\n",
      "Using numerical features: ['rating', 'sentiment_score', 'year', 'month', 'day', 'dayofweek', 'text_length']\n",
      "Training set shape: (3276777, 10)\n",
      "Test set shape: (819195, 10)\n",
      "\n",
      "Training base models with cross-validation...\n",
      "\n",
      "Training random_forest...\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 221\u001b[39m\n\u001b[32m    218\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m trained_models, cv_results\n\u001b[32m    220\u001b[39m \u001b[38;5;66;03m# Train base models\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m trained_models, cv_results = \u001b[43mtrain_base_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocessor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[38;5;66;03m# 5. Evaluate Base Models\u001b[39;00m\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluate_models\u001b[39m(models, X_test, y_test):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 206\u001b[39m, in \u001b[36mtrain_base_models\u001b[39m\u001b[34m(X_train, y_train, preprocessor, cv)\u001b[39m\n\u001b[32m    196\u001b[39m grid_search = GridSearchCV(\n\u001b[32m    197\u001b[39m     pipeline,\n\u001b[32m    198\u001b[39m     param_grids[name],\n\u001b[32m   (...)\u001b[39m\u001b[32m    202\u001b[39m     verbose=\u001b[32m1\u001b[39m\n\u001b[32m    203\u001b[39m )\n\u001b[32m    205\u001b[39m \u001b[38;5;66;03m# Fit the grid search\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m \u001b[43mgrid_search\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[38;5;66;03m# Store the best model\u001b[39;00m\n\u001b[32m    209\u001b[39m trained_models[name] = grid_search.best_estimator_\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1467\u001b[39m     estimator._validate_params()\n\u001b[32m   1469\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1470\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1471\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1472\u001b[39m     )\n\u001b[32m   1473\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1474\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    964\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m    965\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m    966\u001b[39m     )\n\u001b[32m    968\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m--> \u001b[39m\u001b[32m970\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m    973\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m    974\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1527\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1525\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1526\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1527\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:916\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    908\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    909\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    910\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    911\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    912\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    913\u001b[39m         )\n\u001b[32m    914\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m916\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m    935\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    936\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    937\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    938\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    939\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     62\u001b[39m config = get_config()\n\u001b[32m     63\u001b[39m iterable_with_config = (\n\u001b[32m     64\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     66\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1952\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1946\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   1947\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   1948\u001b[39m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[32m   1949\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   1950\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1952\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1595\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1592\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1594\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1595\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1597\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1598\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1599\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1600\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1601\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1707\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1702\u001b[39m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[32m   1703\u001b[39m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[32m   1704\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._jobs) == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m   1705\u001b[39m     (\u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(\n\u001b[32m   1706\u001b[39m         timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING)):\n\u001b[32m-> \u001b[39m\u001b[32m1707\u001b[39m     time.sleep(\u001b[32m0.01\u001b[39m)\n\u001b[32m   1708\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1710\u001b[39m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[32m   1711\u001b[39m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[32m   1712\u001b[39m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 1. Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "# ML Libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "# For RAG system\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "\n",
    "# Create directories for saving models and data\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# 2. Data Loading and Initial Exploration\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load and perform initial data exploration\"\"\"\n",
    "    print(f\"Loading data from {file_path}...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(\"\\nColumn types:\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    print(\"\\nSample data:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    print(\"\\nMissing values per column:\")\n",
    "    print(df.isna().sum())\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the dataset\n",
    "df = load_data('data_set.csv')  # Update with your actual path if needed\n",
    "\n",
    "# 3. Data Preprocessing\n",
    "def preprocess_data(df, target_column='log_price', test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Preprocess the data for machine learning\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The input dataset\n",
    "    target_column : str\n",
    "        The column to predict\n",
    "    test_size : float\n",
    "        Proportion of data to use for testing\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X_train, X_test, y_train, y_test, preprocessor\n",
    "    \"\"\"\n",
    "    print(\"Starting data preprocessing...\")\n",
    "    \n",
    "    # Convert timestamp to datetime\n",
    "    if 'timestamp' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "        df['month'] = df['timestamp'].dt.month\n",
    "        df['day'] = df['timestamp'].dt.day\n",
    "        df['dayofweek'] = df['timestamp'].dt.dayofweek\n",
    "    \n",
    "    # Extract features from text using text length\n",
    "    if 'text' in df.columns and 'text_length' not in df.columns:\n",
    "        df['text_length'] = df['text'].fillna('').apply(len)\n",
    "    \n",
    "    # Define features for the model\n",
    "    categorical_features = [col for col in ['domain', 'main_category', 'sentiment_category'] \n",
    "                           if col in df.columns]\n",
    "    \n",
    "    numerical_features = [col for col in ['rating', 'sentiment_score', 'year', 'month', 'day', \n",
    "                                         'dayofweek', 'text_length'] \n",
    "                         if col in df.columns]\n",
    "    \n",
    "    print(f\"Using categorical features: {categorical_features}\")\n",
    "    print(f\"Using numerical features: {numerical_features}\")\n",
    "    \n",
    "    # Create preprocessing pipelines for both numeric and categorical data\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    # Combine preprocessing steps\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numerical_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ])\n",
    "    \n",
    "    # Split data into features and target\n",
    "    X = df[numerical_features + categorical_features]\n",
    "    y = df[target_column]\n",
    "    \n",
    "    # Split into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set shape: {X_train.shape}\")\n",
    "    print(f\"Test set shape: {X_test.shape}\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, preprocessor, numerical_features, categorical_features\n",
    "\n",
    "# Run preprocessing\n",
    "X_train, X_test, y_train, y_test, preprocessor, numerical_features, categorical_features = preprocess_data(df)\n",
    "\n",
    "# 4. Define and Train Base Models\n",
    "def train_base_models(X_train, y_train, preprocessor, cv=5):\n",
    "    \"\"\"\n",
    "    Train multiple base models with cross-validation\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : pandas DataFrame\n",
    "        Training features\n",
    "    y_train : pandas Series\n",
    "        Target variable\n",
    "    preprocessor : ColumnTransformer\n",
    "        Feature preprocessing pipeline\n",
    "    cv : int\n",
    "        Number of cross-validation folds\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Dictionary of trained model pipelines\n",
    "    \"\"\"\n",
    "    print(\"\\nTraining base models with cross-validation...\")\n",
    "    \n",
    "    # Define base models\n",
    "    base_models = {\n",
    "        'random_forest': RandomForestRegressor(random_state=42),\n",
    "        'gradient_boosting': GradientBoostingRegressor(random_state=42),\n",
    "        'ridge': Ridge(random_state=42),\n",
    "        'svr': SVR()\n",
    "    }\n",
    "    \n",
    "    # Parameters for grid search\n",
    "    param_grids = {\n",
    "        'random_forest': {\n",
    "            'model__n_estimators': [50, 100],\n",
    "            'model__max_depth': [None, 10, 20]\n",
    "        },\n",
    "        'gradient_boosting': {\n",
    "            'model__n_estimators': [50, 100],\n",
    "            'model__learning_rate': [0.01, 0.1]\n",
    "        },\n",
    "        'ridge': {\n",
    "            'model__alpha': [0.1, 1.0, 10.0]\n",
    "        },\n",
    "        'svr': {\n",
    "            'model__C': [0.1, 1.0, 10.0],\n",
    "            'model__kernel': ['linear', 'rbf']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Train with grid search for each model\n",
    "    trained_models = {}\n",
    "    cv_results = {}\n",
    "    \n",
    "    for name, model in base_models.items():\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        \n",
    "        # Create pipeline with preprocessor and model\n",
    "        pipeline = Pipeline(steps=[\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('model', model)\n",
    "        ])\n",
    "        \n",
    "        # Use grid search to find best parameters\n",
    "        grid_search = GridSearchCV(\n",
    "            pipeline,\n",
    "            param_grids[name],\n",
    "            cv=cv,\n",
    "            scoring='neg_mean_squared_error',\n",
    "            n_jobs=-1,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Fit the grid search\n",
    "        grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Store the best model\n",
    "        trained_models[name] = grid_search.best_estimator_\n",
    "        cv_results[name] = {\n",
    "            'best_score': -grid_search.best_score_,  # Convert back to MSE\n",
    "            'best_params': grid_search.best_params_\n",
    "        }\n",
    "        \n",
    "        print(f\"Best {name} score (MSE): {-grid_search.best_score_:.4f}\")\n",
    "        print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "    \n",
    "    return trained_models, cv_results\n",
    "\n",
    "# Train base models\n",
    "trained_models, cv_results = train_base_models(X_train, y_train, preprocessor)\n",
    "\n",
    "# 5. Evaluate Base Models\n",
    "def evaluate_models(models, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate models on test data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    models : dict\n",
    "        Dictionary of trained model pipelines\n",
    "    X_test : pandas DataFrame\n",
    "        Test features\n",
    "    y_test : pandas Series\n",
    "        Test target variable\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    print(\"\\nEvaluating models on test data...\")\n",
    "    \n",
    "    evaluation = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nEvaluating {name}...\")\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        evaluation[name] = {\n",
    "            'MSE': mse,\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'R2': r2\n",
    "        }\n",
    "        \n",
    "        print(f\"MSE: {mse:.4f}\")\n",
    "        print(f\"RMSE: {rmse:.4f}\")\n",
    "        print(f\"MAE: {mae:.4f}\")\n",
    "        print(f\"R2: {r2:.4f}\")\n",
    "        \n",
    "        # Plot actual vs predicted\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "        plt.xlabel('Actual')\n",
    "        plt.ylabel('Predicted')\n",
    "        plt.title(f'{name} - Actual vs Predicted')\n",
    "        plt.show()\n",
    "    \n",
    "    return evaluation\n",
    "\n",
    "# Evaluate base models\n",
    "evaluation = evaluate_models(trained_models, X_test, y_test)\n",
    "\n",
    "# 6. Build Stacking Model (Supervisor)\n",
    "def build_stacking_model(base_models, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Build a stacking model that combines base models\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_models : dict\n",
    "        Dictionary of trained base models\n",
    "    X_train, y_train : Training data\n",
    "    X_test, y_test : Test data\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Trained stacking model and its evaluation\n",
    "    \"\"\"\n",
    "    print(\"\\nBuilding stacking model (supervisor)...\")\n",
    "    \n",
    "    # Prepare base estimators for stacking\n",
    "    estimators = [(name, model) for name, model in base_models.items()]\n",
    "    \n",
    "    # Create stacking regressor\n",
    "    stacking_regressor = StackingRegressor(\n",
    "        estimators=estimators,\n",
    "        final_estimator=Ridge(random_state=42),\n",
    "        cv=5,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Train stacking model\n",
    "    stacking_regressor.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate stacking model\n",
    "    y_pred = stacking_regressor.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    stacking_evaluation = {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2': r2\n",
    "    }\n",
    "    \n",
    "    print(\"\\nStacking Model Evaluation:\")\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"R2: {r2:.4f}\")\n",
    "    \n",
    "    # Plot actual vs predicted for stacking model\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.xlabel('Actual')\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.title('Stacking Model - Actual vs Predicted')\n",
    "    plt.show()\n",
    "    \n",
    "    # Compare all models\n",
    "    all_evaluations = evaluation.copy()\n",
    "    all_evaluations['stacking'] = stacking_evaluation\n",
    "    \n",
    "    metrics = ['RMSE', 'MAE', 'R2']\n",
    "    for metric in metrics:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        model_names = list(all_evaluations.keys())\n",
    "        metric_values = [all_evaluations[model][metric] for model in model_names]\n",
    "        \n",
    "        # Create bar plot\n",
    "        sns.barplot(x=model_names, y=metric_values)\n",
    "        plt.title(f'Comparison of Models - {metric}')\n",
    "        plt.ylabel(metric)\n",
    "        plt.xlabel('Model')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    return stacking_regressor, stacking_evaluation\n",
    "\n",
    "# Build stacking model\n",
    "stacking_model, stacking_evaluation = build_stacking_model(trained_models, X_train, y_train, X_test, y_test)\n",
    "\n",
    "# 7. Save Models and Preprocessing Pipeline\n",
    "def save_pipeline(base_models, stacking_model, preprocessor, \n",
    "                 numerical_features, categorical_features, \n",
    "                 cv_results, evaluation, stacking_evaluation):\n",
    "    \"\"\"\n",
    "    Save all components of the machine learning pipeline\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    Various model components and metadata\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Dictionary of saved file paths\n",
    "    \"\"\"\n",
    "    print(\"\\nSaving machine learning pipeline components...\")\n",
    "    \n",
    "    saved_files = {}\n",
    "    \n",
    "    # Save base models\n",
    "    for name, model in base_models.items():\n",
    "        model_path = f'models/{name}_model.pkl'\n",
    "        joblib.dump(model, model_path)\n",
    "        saved_files[f'{name}_model'] = model_path\n",
    "    \n",
    "    # Save stacking model\n",
    "    stacking_path = 'models/stacking_model.pkl'\n",
    "    joblib.dump(stacking_model, stacking_path)\n",
    "    saved_files['stacking_model'] = stacking_path\n",
    "    \n",
    "    # Save preprocessor\n",
    "    preprocessor_path = 'models/preprocessor.pkl'\n",
    "    joblib.dump(preprocessor, preprocessor_path)\n",
    "    saved_files['preprocessor'] = preprocessor_path\n",
    "    \n",
    "    # Save feature lists\n",
    "    features = {\n",
    "        'numerical_features': numerical_features,\n",
    "        'categorical_features': categorical_features\n",
    "    }\n",
    "    features_path = 'models/feature_lists.pkl'\n",
    "    with open(features_path, 'wb') as f:\n",
    "        pickle.dump(features, f)\n",
    "    saved_files['features'] = features_path\n",
    "    \n",
    "    # Save evaluation results\n",
    "    results = {\n",
    "        'cv_results': cv_results,\n",
    "        'evaluation': evaluation,\n",
    "        'stacking_evaluation': stacking_evaluation\n",
    "    }\n",
    "    results_path = 'models/evaluation_results.pkl'\n",
    "    with open(results_path, 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "    saved_files['results'] = results_path\n",
    "    \n",
    "    # Save model metadata\n",
    "    metadata = {\n",
    "        'creation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'model_names': list(base_models.keys()),\n",
    "        'feature_count': len(numerical_features) + len(categorical_features),\n",
    "        'numerical_features': numerical_features,\n",
    "        'categorical_features': categorical_features,\n",
    "        'target': 'log_price'\n",
    "    }\n",
    "    metadata_path = 'models/model_metadata.pkl'\n",
    "    with open(metadata_path, 'wb') as f:\n",
    "        pickle.dump(metadata, f)\n",
    "    saved_files['metadata'] = metadata_path\n",
    "    \n",
    "    print(\"Pipeline components saved successfully!\")\n",
    "    return saved_files\n",
    "\n",
    "# Save pipeline\n",
    "saved_files = save_pipeline(\n",
    "    trained_models, \n",
    "    stacking_model, \n",
    "    preprocessor, \n",
    "    numerical_features, \n",
    "    categorical_features, \n",
    "    cv_results, \n",
    "    evaluation, \n",
    "    stacking_evaluation\n",
    ")\n",
    "\n",
    "# 8. Load Models and Make Predictions\n",
    "def load_pipeline():\n",
    "    \"\"\"\n",
    "    Load all components of the saved machine learning pipeline\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Dictionary of loaded pipeline components\n",
    "    \"\"\"\n",
    "    print(\"\\nLoading machine learning pipeline components...\")\n",
    "    \n",
    "    pipeline = {}\n",
    "    \n",
    "    # Load metadata to get model names\n",
    "    with open('models/model_metadata.pkl', 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    pipeline['metadata'] = metadata\n",
    "    \n",
    "    # Load base models\n",
    "    base_models = {}\n",
    "    for name in metadata['model_names']:\n",
    "        model_path = f'models/{name}_model.pkl'\n",
    "        base_models[name] = joblib.load(model_path)\n",
    "    pipeline['base_models'] = base_models\n",
    "    \n",
    "    # Load stacking model\n",
    "    pipeline['stacking_model'] = joblib.load('models/stacking_model.pkl')\n",
    "    \n",
    "    # Load preprocessor\n",
    "    pipeline['preprocessor'] = joblib.load('models/preprocessor.pkl')\n",
    "    \n",
    "    # Load feature lists\n",
    "    with open('models/feature_lists.pkl', 'rb') as f:\n",
    "        features = pickle.load(f)\n",
    "    pipeline['numerical_features'] = features['numerical_features']\n",
    "    pipeline['categorical_features'] = features['categorical_features']\n",
    "    \n",
    "    # Load evaluation results\n",
    "    with open('models/evaluation_results.pkl', 'rb') as f:\n",
    "        results = pickle.load(f)\n",
    "    pipeline['cv_results'] = results['cv_results']\n",
    "    pipeline['evaluation'] = results['evaluation']\n",
    "    pipeline['stacking_evaluation'] = results['stacking_evaluation']\n",
    "    \n",
    "    print(\"Pipeline components loaded successfully!\")\n",
    "    return pipeline\n",
    "\n",
    "def make_predictions(df, pipeline, return_all_predictions=False):\n",
    "    \"\"\"\n",
    "    Make predictions using the loaded pipeline\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        New data for prediction\n",
    "    pipeline : dict\n",
    "        Loaded pipeline components\n",
    "    return_all_predictions : bool\n",
    "        Whether to return predictions from all models or just the stacking model\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with predictions\n",
    "    \"\"\"\n",
    "    print(\"\\nPreparing data for prediction...\")\n",
    "    \n",
    "    # Ensure we have all required features\n",
    "    numerical_features = pipeline['numerical_features']\n",
    "    categorical_features = pipeline['categorical_features']\n",
    "    \n",
    "    # Modified timestamp conversion in preprocess_data function\n",
    "    if 'timestamp' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "        df['month'] = df['timestamp'].dt.month\n",
    "        df['day'] = df['timestamp'].dt.day\n",
    "        df['dayofweek'] = df['timestamp'].dt.dayofweek\n",
    "    \n",
    "    # Create text_length if text exists and text_length doesn't\n",
    "    if 'text' in df.columns and 'text_length' not in df.columns:\n",
    "        df['text_length'] = df['text'].fillna('').apply(len)\n",
    "    \n",
    "    # Select required features\n",
    "    X = df[numerical_features + categorical_features].copy()\n",
    "    \n",
    "    print(\"Making predictions...\")\n",
    "    \n",
    "    # Make predictions with each base model\n",
    "    predictions = {}\n",
    "    for name, model in pipeline['base_models'].items():\n",
    "        predictions[f'{name}_prediction'] = model.predict(X)\n",
    "    \n",
    "    # Make predictions with stacking model\n",
    "    predictions['stacking_prediction'] = pipeline['stacking_model'].predict(X)\n",
    "    \n",
    "    # Add predictions to the original dataframe\n",
    "    results_df = df.copy()\n",
    "    for name, preds in predictions.items():\n",
    "        results_df[name] = preds\n",
    "    \n",
    "    # Convert log_price predictions back to price if requested\n",
    "    if 'log_price' in results_df.columns:\n",
    "        for name in predictions.keys():\n",
    "            orig_price_col = name.replace('prediction', 'orig_price')\n",
    "            results_df[orig_price_col] = np.exp(results_df[name])\n",
    "    \n",
    "    print(\"Predictions completed!\")\n",
    "    \n",
    "    if return_all_predictions:\n",
    "        return results_df\n",
    "    else:\n",
    "        # Return only the stacking model predictions\n",
    "        selected_cols = list(df.columns) + ['stacking_prediction']\n",
    "        if 'log_price' in results_df.columns:\n",
    "            selected_cols.append('stacking_orig_price')\n",
    "        return results_df[selected_cols]\n",
    "\n",
    "# 9. Implement RAG (Retrieval-Augmented Generation) System\n",
    "def create_knowledge_base(df):\n",
    "    \"\"\"\n",
    "    Create a knowledge base from the dataset for retrieval\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        Dataset with text content\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    TF-IDF vectorizer and document vectors\n",
    "    \"\"\"\n",
    "    print(\"\\nCreating knowledge base from dataset...\")\n",
    "    \n",
    "    # Prepare text content for knowledge base\n",
    "    # Combine relevant columns into a single text field\n",
    "    if 'text' in df.columns:\n",
    "        # Create a combined text field with metadata\n",
    "        df['kb_text'] = df.apply(\n",
    "            lambda row: f\"Product: {row.get('asin', 'Unknown')} | \"\n",
    "                      + f\"Category: {row.get('main_category', 'Unknown')} | \"\n",
    "                      + f\"Rating: {row.get('rating', 'Unknown')} | \"\n",
    "                      + f\"Price: {np.exp(row['log_price']) if 'log_price' in df.columns else 'Unknown'} | \"\n",
    "                      + f\"Sentiment: {row.get('sentiment_category', 'Unknown')} | \"\n",
    "                      + f\"Text: {row.get('text', '')}\",\n",
    "            axis=1\n",
    "        )\n",
    "    else:\n",
    "        # If no text column, create metadata-only knowledge base\n",
    "        df['kb_text'] = df.apply(\n",
    "            lambda row: f\"Product: {row.get('asin', 'Unknown')} | \"\n",
    "                      + f\"Category: {row.get('main_category', 'Unknown')} | \"\n",
    "                      + f\"Rating: {row.get('rating', 'Unknown')} | \"\n",
    "                      + f\"Price: {np.exp(row['log_price']) if 'log_price' in df.columns else 'Unknown'} | \"\n",
    "                      + f\"Sentiment: {row.get('sentiment_category', 'Unknown')}\",\n",
    "            axis=1\n",
    "        )\n",
    "    \n",
    "    # Create TF-IDF vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "    \n",
    "    # Create document vectors\n",
    "    doc_vectors = tfidf_vectorizer.fit_transform(df['kb_text'])\n",
    "    \n",
    "    print(f\"Knowledge base created with {doc_vectors.shape[0]} documents and {doc_vectors.shape[1]} features\")\n",
    "    \n",
    "    return tfidf_vectorizer, doc_vectors, df['kb_text'].tolist()\n",
    "\n",
    "def retrieve_relevant_documents(query, vectorizer, doc_vectors, documents, top_n=5):\n",
    "    \"\"\"\n",
    "    Retrieve relevant documents based on the query\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    query : str\n",
    "        User query\n",
    "    vectorizer : TfidfVectorizer\n",
    "        Fitted vectorizer\n",
    "    doc_vectors : sparse matrix\n",
    "        Document vectors\n",
    "    documents : list\n",
    "        Original documents\n",
    "    top_n : int\n",
    "        Number of documents to retrieve\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    List of relevant documents\n",
    "    \"\"\"\n",
    "    # Vectorize the query\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarities = cosine_similarity(query_vector, doc_vectors).flatten()\n",
    "    \n",
    "    # Get indices of top similar documents\n",
    "    top_indices = similarities.argsort()[-top_n:][::-1]\n",
    "    \n",
    "    # Get top documents and their similarity scores\n",
    "    top_docs = [(documents[i], similarities[i]) for i in top_indices]\n",
    "    \n",
    "    return top_docs\n",
    "\n",
    "# 10. Integration with Chatbot Functions\n",
    "def generate_prediction_response(query, df_predictions, knowledge_base):\n",
    "    \"\"\"\n",
    "    Generate a response based on the query and predictions\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    query : str\n",
    "        User query\n",
    "    df_predictions : pandas DataFrame\n",
    "        DataFrame with predictions\n",
    "    knowledge_base : tuple\n",
    "        Knowledge base components (vectorizer, doc_vectors, documents)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Response text to be used by the chatbot\n",
    "    \"\"\"\n",
    "    vectorizer, doc_vectors, documents = knowledge_base\n",
    "    \n",
    "    # Retrieve relevant documents\n",
    "    relevant_docs = retrieve_relevant_documents(query, vectorizer, doc_vectors, documents)\n",
    "    \n",
    "    # Generate context from relevant documents\n",
    "    context = \"\\n\\n\".join([f\"Document (similarity: {score:.2f}):\\n{doc}\" \n",
    "                          for doc, score in relevant_docs])\n",
    "    \n",
    "    # Extract prediction stats\n",
    "    avg_price = np.exp(df_predictions['stacking_prediction'].mean())\n",
    "    min_price = np.exp(df_predictions['stacking_prediction'].min())\n",
    "    max_price = np.exp(df_predictions['stacking_prediction'].max())\n",
    "    \n",
    "    prediction_stats = f\"\"\"\n",
    "    Prediction Statistics:\n",
    "    - Average predicted price: ${avg_price:.2f}\n",
    "    - Minimum predicted price: ${min_price:.2f}\n",
    "    - Maximum predicted price: ${max_price:.2f}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Combine context and prediction stats to form the response\n",
    "    response = f\"\"\"\n",
    "    Based on your query: \"{query}\"\n",
    "    \n",
    "    {prediction_stats}\n",
    "    \n",
    "    Relevant information from the knowledge base:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "    \n",
    "    return response\n",
    "\n",
    "def integrate_with_chatbot(df_predictions):\n",
    "    \"\"\"\n",
    "    Integrate the prediction system with existing chatbot functions\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_predictions : pandas DataFrame\n",
    "        DataFrame with predictions\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    None - this function calls the chatbot functions\n",
    "    \"\"\"\n",
    "    print(\"\\nIntegrating prediction system with chatbot...\")\n",
    "    \n",
    "    # Create knowledge base\n",
    "    knowledge_base = create_knowledge_base(df_predictions)\n",
    "    \n",
    "    # Define a wrapper function to be used by the chatbot\n",
    "    def prediction_chatbot_handler(query):\n",
    "        # Generate response based on the query and predictions\n",
    "        response = generate_prediction_response(query, df_predictions, knowledge_base)\n",
    "        return response\n",
    "    \n",
    "    # Set up the prediction response generator\n",
    "    print(\"Setting up prediction response generator...\")\n",
    "    # This would be integrated with your existing generate_chat_response_v2 function\n",
    "    \n",
    "    # Start the audio chatbot\n",
    "    print(\"\\nStarting audio chatbot interaction...\")\n",
    "    print(\"Use '1' to start and '2' to stop the chatbot.\")\n",
    "    # This would call your existing run_audio_chatbot_v2 function\n",
    "    \n",
    "    # Note: The actual function calls are commented out since these are external functions\n",
    "    # In real implementation, you would uncomment these lines\n",
    "    # run_audio_chatbot_v2(start_trigger='1', stop_trigger='2', response_generator=prediction_chatbot_handler)\n",
    "    \n",
    "    print(\"\\nChatbot integration complete!\")\n",
    "\n",
    "# 11. Main Execution Function\n",
    "def run_prediction_pipeline(data_path='data_set.csv', retrain=False):\n",
    "    \"\"\"\n",
    "    Run the complete prediction pipeline\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_path : str\n",
    "        Path to the input CSV file\n",
    "    retrain : bool\n",
    "        Whether to retrain the models or load existing ones\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with predictions\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    df = load_data(data_path)\n",
    "    \n",
    "    if retrain:\n",
    "        # Preprocess data\n",
    "        X_train, X_test, y_train, y_test, preprocessor, numerical_features, categorical_features = preprocess_data(df)\n",
    "        \n",
    "        # Train base models\n",
    "        trained_models, cv_results = train_base_models(X_train, y_train, preprocessor)\n",
    "        \n",
    "        # Evaluate base models\n",
    "        evaluation = evaluate_models(trained_models, X_test, y_test)\n",
    "        \n",
    "        # Build stacking model\n",
    "        stacking_model, stacking_evaluation = build_stacking_model(\n",
    "            trained_models, X_train, y_train, X_test, y_test\n",
    "        )\n",
    "        \n",
    "        # Save pipeline\n",
    "        saved_files = save_pipeline(\n",
    "            trained_models, \n",
    "            stacking_model, \n",
    "            preprocessor, \n",
    "            numerical_features, \n",
    "            categorical_features, \n",
    "            cv_results, \n",
    "            evaluation, \n",
    "            stacking_evaluation\n",
    "        )\n",
    "        \n",
    "        # Make predictions\n",
    "        pipeline = {\n",
    "            'base_models': trained_models,\n",
    "            'stacking_model': stacking_model,\n",
    "            'preprocessor': preprocessor,\n",
    "            'numerical_features': numerical_features,\n",
    "            'categorical_features': categorical_features\n",
    "        }\n",
    "        predictions_df = make_predictions(df, pipeline, return_all_predictions=True)\n",
    "        \n",
    "    else:\n",
    "        # Load existing pipeline\n",
    "        pipeline = load_pipeline()\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions_df = make_predictions(df, pipeline, return_all_predictions=True)\n",
    "    \n",
    "    # Integrate with chatbot\n",
    "    integrate_with_chatbot(predictions_df)\n",
    "    \n",
    "    return predictions_df\n",
    "\n",
    "# Run the complete pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    predictions_df = run_prediction_pipeline(retrain=True)\n",
    "    print(\"\\nPipeline execution complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a80c480",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Loading data from data_set.csv...\n",
      "Dataset shape: (4095972, 13)\n",
      "\n",
      "Column types:\n",
      "parent_asin            object\n",
      "asin                   object\n",
      "rating                  int64\n",
      "timestamp              object\n",
      "text                   object\n",
      "main_category          object\n",
      "price                 float64\n",
      "log_price             float64\n",
      "domain                 object\n",
      "sentiment_score       float64\n",
      "sentiment_category     object\n",
      "year                    int64\n",
      "text_length             int64\n",
      "dtype: object\n",
      "\n",
      "Sample data:\n",
      "  parent_asin        asin  rating            timestamp  \\\n",
      "0  B000000LWZ  B000000LWZ       5  1997-09-09 03:13:17   \n",
      "1  B000002H3Z  B000002H3Z       5  1997-09-13 22:05:50   \n",
      "2  B000003YSX  B000003YSX       5  1997-10-05 21:21:20   \n",
      "3  B000002H2H  B000002H2H       5  1997-10-20 14:48:57   \n",
      "4  B000002AS3  B000002AS3       5  1997-10-21 17:42:32   \n",
      "\n",
      "                                                text  main_category  price  \\\n",
      "0  It was while driving on Highway 83, heading of...  Digital Music  10.98   \n",
      "1  This isn't actually a book. It is a beautifull...  Digital Music  14.98   \n",
      "2  This album is not of what one would usually re...  Digital Music  15.99   \n",
      "3  Released on November 23 1984, Metallica's seco...  Digital Music   7.30   \n",
      "4  This is the first cd made by Oasis. My favorit...  Digital Music   9.09   \n",
      "\n",
      "   log_price         domain  sentiment_score sentiment_category  year  \\\n",
      "0   2.396075  CDs_and_Vinyl         0.256641           Positive  1997   \n",
      "1   2.706716  CDs_and_Vinyl         0.620000           Positive  1997   \n",
      "2   2.771964  CDs_and_Vinyl         0.113333            Neutral  1997   \n",
      "3   1.987874  CDs_and_Vinyl         0.149953            Neutral  1997   \n",
      "4   2.207175  CDs_and_Vinyl         0.417614           Positive  1997   \n",
      "\n",
      "   text_length  \n",
      "0         1029  \n",
      "1          337  \n",
      "2          754  \n",
      "3         1289  \n",
      "4          179  \n",
      "\n",
      "Missing values per column:\n",
      "parent_asin           0\n",
      "asin                  0\n",
      "rating                0\n",
      "timestamp             0\n",
      "text                  0\n",
      "main_category         0\n",
      "price                 0\n",
      "log_price             0\n",
      "domain                0\n",
      "sentiment_score       0\n",
      "sentiment_category    0\n",
      "year                  0\n",
      "text_length           0\n",
      "dtype: int64\n",
      "Starting data preprocessing...\n",
      "Using categorical features: ['domain', 'main_category', 'sentiment_category']\n",
      "Using numerical features: ['rating', 'sentiment_score', 'year', 'month', 'day', 'dayofweek', 'text_length']\n",
      "Training set shape: (3276777, 10)\n",
      "Test set shape: (819195, 10)\n",
      "\n",
      "Training base models with cross-validation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Model Training Progress:   0%|          | 0/4 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training random_forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overall Model Training Progress:   0%|          | 0/4 [5:02:59<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 277\u001b[39m\n\u001b[32m    274\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m trained_models, cv_results\n\u001b[32m    276\u001b[39m \u001b[38;5;66;03m# Train base models\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m trained_models, cv_results = \u001b[43mtrain_base_models\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocessor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[38;5;66;03m# 5. Evaluate Base Models\u001b[39;00m\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mevaluate_models\u001b[39m(models, X_test, y_test):\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 246\u001b[39m, in \u001b[36mtrain_base_models\u001b[39m\u001b[34m(X_train, y_train, preprocessor, cv)\u001b[39m\n\u001b[32m    244\u001b[39m \u001b[38;5;66;03m# Fit the grid search while manually updating progress\u001b[39;00m\n\u001b[32m    245\u001b[39m start_time = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m246\u001b[39m \u001b[43mgrid_search\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    247\u001b[39m train_time = time.time() - start_time\n\u001b[32m    249\u001b[39m \u001b[38;5;66;03m# Update progress bar to completion\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1467\u001b[39m     estimator._validate_params()\n\u001b[32m   1469\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1470\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1471\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1472\u001b[39m     )\n\u001b[32m   1473\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1474\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[39m, in \u001b[36mBaseSearchCV.fit\u001b[39m\u001b[34m(self, X, y, **params)\u001b[39m\n\u001b[32m    964\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._format_results(\n\u001b[32m    965\u001b[39m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[32m    966\u001b[39m     )\n\u001b[32m    968\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[32m--> \u001b[39m\u001b[32m970\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    972\u001b[39m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[32m    973\u001b[39m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[32m    974\u001b[39m first_test_score = all_out[\u001b[32m0\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtest_scores\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1527\u001b[39m, in \u001b[36mGridSearchCV._run_search\u001b[39m\u001b[34m(self, evaluate_candidates)\u001b[39m\n\u001b[32m   1525\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[32m   1526\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1527\u001b[39m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:916\u001b[39m, in \u001b[36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[39m\u001b[34m(candidate_params, cv, more_results)\u001b[39m\n\u001b[32m    908\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.verbose > \u001b[32m0\u001b[39m:\n\u001b[32m    909\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m    910\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[33m candidates,\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    911\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[33m fits\u001b[39m\u001b[33m\"\u001b[39m.format(\n\u001b[32m    912\u001b[39m             n_splits, n_candidates, n_candidates * n_splits\n\u001b[32m    913\u001b[39m         )\n\u001b[32m    914\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m916\u001b[39m out = \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    921\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    922\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    923\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    924\u001b[39m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    925\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    926\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    927\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    928\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    929\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mrouted_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    931\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    932\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    934\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) < \u001b[32m1\u001b[39m:\n\u001b[32m    935\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    936\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mNo fits were performed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    937\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWas the CV iterator empty? \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    938\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mWere there no candidates?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    939\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:67\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     62\u001b[39m config = get_config()\n\u001b[32m     63\u001b[39m iterable_with_config = (\n\u001b[32m     64\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     66\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1952\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   1946\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   1947\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   1948\u001b[39m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[32m   1949\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   1950\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m1952\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1595\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1592\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1594\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1595\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1597\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1598\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1599\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1600\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1601\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ahmed Mohamed\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1707\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1702\u001b[39m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[32m   1703\u001b[39m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[32m   1704\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._jobs) == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m   1705\u001b[39m     (\u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(\n\u001b[32m   1706\u001b[39m         timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING)):\n\u001b[32m-> \u001b[39m\u001b[32m1707\u001b[39m     time.sleep(\u001b[32m0.01\u001b[39m)\n\u001b[32m   1708\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1710\u001b[39m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[32m   1711\u001b[39m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[32m   1712\u001b[39m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# 1. Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import os\n",
    "import joblib\n",
    "from tqdm import tqdm \n",
    "import time\n",
    "# ML Libraries\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "# For RAG system\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(\"Libraries imported successfully!\")\n",
    "\n",
    "# Create directories for saving models and data\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('data', exist_ok=True)\n",
    "\n",
    "# 2. Data Loading and Initial Exploration\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load and perform initial data exploration\"\"\"\n",
    "    print(f\"Loading data from {file_path}...\")\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    print(\"\\nColumn types:\")\n",
    "    print(df.dtypes)\n",
    "    \n",
    "    print(\"\\nSample data:\")\n",
    "    print(df.head())\n",
    "    \n",
    "    print(\"\\nMissing values per column:\")\n",
    "    print(df.isna().sum())\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load the dataset\n",
    "df = load_data('data_set.csv')  # Update with your actual path if needed\n",
    "\n",
    "# 3. Data Preprocessing\n",
    "def preprocess_data(df, target_column='log_price', test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Preprocess the data for machine learning\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        The input dataset\n",
    "    target_column : str\n",
    "        The column to predict\n",
    "    test_size : float\n",
    "        Proportion of data to use for testing\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    X_train, X_test, y_train, y_test, preprocessor\n",
    "    \"\"\"\n",
    "    print(\"Starting data preprocessing...\")\n",
    "    \n",
    "    # Convert timestamp to datetime\n",
    "    if 'timestamp' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "        df['month'] = df['timestamp'].dt.month\n",
    "        df['day'] = df['timestamp'].dt.day\n",
    "        df['dayofweek'] = df['timestamp'].dt.dayofweek\n",
    "    \n",
    "    # Extract features from text using text length\n",
    "    if 'text' in df.columns and 'text_length' not in df.columns:\n",
    "        df['text_length'] = df['text'].fillna('').apply(len)\n",
    "    \n",
    "    # Define features for the model\n",
    "    categorical_features = [col for col in ['domain', 'main_category', 'sentiment_category'] \n",
    "                           if col in df.columns]\n",
    "    \n",
    "    numerical_features = [col for col in ['rating', 'sentiment_score', 'year', 'month', 'day', \n",
    "                                         'dayofweek', 'text_length'] \n",
    "                         if col in df.columns]\n",
    "    \n",
    "    print(f\"Using categorical features: {categorical_features}\")\n",
    "    print(f\"Using numerical features: {numerical_features}\")\n",
    "    \n",
    "    # Create preprocessing pipelines for both numeric and categorical data\n",
    "    numeric_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='median')),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "    \n",
    "    categorical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "    ])\n",
    "    \n",
    "    # Combine preprocessing steps\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('num', numeric_transformer, numerical_features),\n",
    "            ('cat', categorical_transformer, categorical_features)\n",
    "        ])\n",
    "    \n",
    "    # Split data into features and target\n",
    "    X = df[numerical_features + categorical_features]\n",
    "    y = df[target_column]\n",
    "    \n",
    "    # Split into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set shape: {X_train.shape}\")\n",
    "    print(f\"Test set shape: {X_test.shape}\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, preprocessor, numerical_features, categorical_features\n",
    "\n",
    "# Run preprocessing\n",
    "X_train, X_test, y_train, y_test, preprocessor, numerical_features, categorical_features = preprocess_data(df)\n",
    "\n",
    "# 4. Define and Train Base Models\n",
    "def train_base_models(X_train, y_train, preprocessor, cv=5):\n",
    "    \"\"\"\n",
    "    Train multiple base models with cross-validation and detailed progress tracking\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : pandas DataFrame\n",
    "        Training features\n",
    "    y_train : pandas Series\n",
    "        Target variable\n",
    "    preprocessor : ColumnTransformer\n",
    "        Feature preprocessing pipeline\n",
    "    cv : int\n",
    "        Number of cross-validation folds\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Dictionary of trained model pipelines\n",
    "    \"\"\"\n",
    "    print(\"\\nTraining base models with cross-validation...\")\n",
    "    \n",
    "    # Define base models\n",
    "    base_models = {\n",
    "        'random_forest': RandomForestRegressor(random_state=42),\n",
    "        'gradient_boosting': GradientBoostingRegressor(random_state=42),\n",
    "        'ridge': Ridge(random_state=42),\n",
    "        'svr': SVR()\n",
    "    }\n",
    "    \n",
    "    # Parameters for grid search\n",
    "    param_grids = {\n",
    "        'random_forest': {\n",
    "            'model__n_estimators': [50, 100],\n",
    "            'model__max_depth': [None, 10, 20]\n",
    "        },\n",
    "        'gradient_boosting': {\n",
    "            'model__n_estimators': [50, 100],\n",
    "            'model__learning_rate': [0.01, 0.1]\n",
    "        },\n",
    "        'ridge': {\n",
    "            'model__alpha': [0.1, 1.0, 10.0]\n",
    "        },\n",
    "        'svr': {\n",
    "            'model__C': [0.1, 1.0, 10.0],\n",
    "            'model__kernel': ['linear', 'rbf']\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Create a master progress bar for all models\n",
    "    total_models = len(base_models)\n",
    "    with tqdm(total=total_models, desc=\"Overall Model Training Progress\", position=0) as master_pbar:\n",
    "        \n",
    "        # Train with grid search for each model\n",
    "        trained_models = {}\n",
    "        cv_results = {}\n",
    "        \n",
    "        for name, model in base_models.items():\n",
    "            print(f\"\\nTraining {name}...\")\n",
    "            \n",
    "            # Create pipeline with preprocessor and model\n",
    "            pipeline = Pipeline(steps=[\n",
    "                ('preprocessor', preprocessor),\n",
    "                ('model', model)\n",
    "            ])\n",
    "            \n",
    "            # Calculate total iterations for this model's grid search\n",
    "            total_param_combinations = 1\n",
    "            for param_values in param_grids[name].values():\n",
    "                total_param_combinations *= len(param_values)\n",
    "            total_iterations = total_param_combinations * cv\n",
    "            \n",
    "            # Create dedicated progress bar for this model\n",
    "            model_pbar = tqdm(total=total_iterations, \n",
    "                             desc=f\"{name.capitalize()} Training\", \n",
    "                             position=1, \n",
    "                             leave=False)\n",
    "            \n",
    "            # Define callback for GridSearchCV to update progress\n",
    "            def progress_callback(iteration, total, status=''):\n",
    "                model_pbar.update(1)\n",
    "                \n",
    "            # Define verbose GridSearchCV to capture progress\n",
    "            class VerboseCallback:\n",
    "                def __init__(self, estimator_name, verbose=0):\n",
    "                    self.name = estimator_name\n",
    "                    self.verbose = verbose\n",
    "                    self.n_iter = 0\n",
    "                    \n",
    "                def __call__(self, estimator, fold_idx, param_idx):\n",
    "                    self.n_iter += 1\n",
    "                    if self.verbose > 0:\n",
    "                        progress_callback(self.n_iter, None)\n",
    "                    return estimator\n",
    "            \n",
    "            # Use grid search with callback\n",
    "            grid_search = GridSearchCV(\n",
    "                pipeline,\n",
    "                param_grids[name],\n",
    "                cv=cv,\n",
    "                scoring='neg_mean_squared_error',\n",
    "                n_jobs=-1,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            # Train the model and track iterations\n",
    "            iter_count = 0\n",
    "            for params in grid_search.param_grid:\n",
    "                pass  # Just to initialize the grid\n",
    "            \n",
    "            # Fit the grid search while manually updating progress\n",
    "            start_time = time.time()\n",
    "            grid_search.fit(X_train, y_train)\n",
    "            train_time = time.time() - start_time\n",
    "            \n",
    "            # Update progress bar to completion\n",
    "            model_pbar.update(total_iterations - model_pbar.n)\n",
    "            model_pbar.close()\n",
    "            \n",
    "            # Store the best model\n",
    "            trained_models[name] = grid_search.best_estimator_\n",
    "            cv_results[name] = {\n",
    "                'best_score': -grid_search.best_score_,  # Convert back to MSE\n",
    "                'best_params': grid_search.best_params_,\n",
    "                'training_time': train_time\n",
    "            }\n",
    "            \n",
    "            # Print results for this model\n",
    "            print(f\"Best {name} score (MSE): {-grid_search.best_score_:.4f}\")\n",
    "            print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "            print(f\"Training time: {train_time:.2f} seconds\")\n",
    "            \n",
    "            # Update master progress bar\n",
    "            master_pbar.update(1)\n",
    "    \n",
    "    # Print summary of all models\n",
    "    print(\"\\nModel Training Summary:\")\n",
    "    for name, results in cv_results.items():\n",
    "        print(f\"{name.capitalize()}: MSE = {results['best_score']:.4f}, Time = {results['training_time']:.2f}s\")\n",
    "    \n",
    "    return trained_models, cv_results\n",
    "\n",
    "# Train base models\n",
    "trained_models, cv_results = train_base_models(X_train, y_train, preprocessor)\n",
    "\n",
    "# 5. Evaluate Base Models\n",
    "def evaluate_models(models, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Evaluate models on test data\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    models : dict\n",
    "        Dictionary of trained model pipelines\n",
    "    X_test : pandas DataFrame\n",
    "        Test features\n",
    "    y_test : pandas Series\n",
    "        Test target variable\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Dictionary of evaluation metrics\n",
    "    \"\"\"\n",
    "    print(\"\\nEvaluating models on test data...\")\n",
    "    \n",
    "    evaluation = {}\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nEvaluating {name}...\")\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        evaluation[name] = {\n",
    "            'MSE': mse,\n",
    "            'RMSE': rmse,\n",
    "            'MAE': mae,\n",
    "            'R2': r2\n",
    "        }\n",
    "        \n",
    "        print(f\"MSE: {mse:.4f}\")\n",
    "        print(f\"RMSE: {rmse:.4f}\")\n",
    "        print(f\"MAE: {mae:.4f}\")\n",
    "        print(f\"R2: {r2:.4f}\")\n",
    "        \n",
    "        # Plot actual vs predicted\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "        plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "        plt.xlabel('Actual')\n",
    "        plt.ylabel('Predicted')\n",
    "        plt.title(f'{name} - Actual vs Predicted')\n",
    "        plt.show()\n",
    "    \n",
    "    return evaluation\n",
    "\n",
    "# Evaluate base models\n",
    "evaluation = evaluate_models(trained_models, X_test, y_test)\n",
    "\n",
    "# 6. Build Stacking Model (Supervisor)\n",
    "def build_stacking_model(base_models, X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Build a stacking model that combines base models with detailed progress tracking\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    base_models : dict\n",
    "        Dictionary of trained base models\n",
    "    X_train, y_train : Training data\n",
    "    X_test, y_test : Test data\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Trained stacking model and its evaluation\n",
    "    \"\"\"\n",
    "    print(\"\\nBuilding stacking model (supervisor)...\")\n",
    "    \n",
    "    # Prepare base estimators for stacking\n",
    "    estimators = [(name, model) for name, model in base_models.items()]\n",
    "    \n",
    "    # Create stacking regressor\n",
    "    stacking_regressor = StackingRegressor(\n",
    "        estimators=estimators,\n",
    "        final_estimator=Ridge(random_state=42),\n",
    "        cv=5,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Create progress bar for each phase of stacking\n",
    "    phases = [\"Base Models Cross-Validation\", \"Final Estimator Training\"]\n",
    "    weights = [80, 20]  # Approximate weights of each phase\n",
    "    \n",
    "    # Overall progress bar\n",
    "    with tqdm(total=100, desc=\"Stacking Model Training\", position=0) as pbar:\n",
    "        # Phase progress bar\n",
    "        with tqdm(total=weights[0], desc=phases[0], position=1, leave=False) as phase_pbar:\n",
    "            # Custom fit method with progress tracking\n",
    "            # We'll use a monkey-patched cross_val_predict to track progress\n",
    "            original_cross_val = stacking_regressor._fit_estimators\n",
    "            \n",
    "            def tracked_cross_val(*args, **kwargs):\n",
    "                # Update progress as cross-validation proceeds\n",
    "                result = original_cross_val(*args, **kwargs)\n",
    "                phase_pbar.update(weights[0])\n",
    "                pbar.update(weights[0])\n",
    "                return result\n",
    "            \n",
    "            # Replace method temporarily\n",
    "            stacking_regressor._fit_estimators = tracked_cross_val\n",
    "            \n",
    "            # Start timing\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Begin phase 1\n",
    "            phase_time = time.time()\n",
    "            phase1_result = stacking_regressor._fit_estimators(X_train, y_train)\n",
    "            phase1_time = time.time() - phase_time\n",
    "            \n",
    "        # Phase 2 progress bar\n",
    "        with tqdm(total=weights[1], desc=phases[1], position=1, leave=False) as phase_pbar:\n",
    "            # Begin phase 2: Final estimator training\n",
    "            phase_time = time.time()\n",
    "            stacking_regressor.final_estimator_ = stacking_regressor.final_estimator.fit(\n",
    "                stacking_regressor._predict(X_train), y_train\n",
    "            )\n",
    "            phase2_time = time.time() - phase_time\n",
    "            \n",
    "            # Update progress\n",
    "            phase_pbar.update(weights[1])\n",
    "            pbar.update(weights[1])\n",
    "        \n",
    "        # Restore original method\n",
    "        stacking_regressor._fit_estimators = original_cross_val\n",
    "        \n",
    "        # Total training time\n",
    "        total_time = time.time() - start_time\n",
    "    \n",
    "    # Print training time statistics\n",
    "    print(f\"\\nStacking model training completed in {total_time:.2f} seconds\")\n",
    "    print(f\"- Base models cross-validation: {phase1_time:.2f} seconds\")\n",
    "    print(f\"- Final estimator training: {phase2_time:.2f} seconds\")\n",
    "    \n",
    "    # Evaluate stacking model\n",
    "    y_pred = stacking_regressor.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    stacking_evaluation = {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R2': r2,\n",
    "        'training_time': total_time\n",
    "    }\n",
    "    \n",
    "    print(\"\\nStacking Model Evaluation:\")\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"R2: {r2:.4f}\")\n",
    "    \n",
    "    # Plot actual vs predicted for stacking model\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.xlabel('Actual')\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.title('Stacking Model - Actual vs Predicted')\n",
    "    plt.show()\n",
    "    \n",
    "    # Compare all models\n",
    "    all_evaluations = evaluation.copy()\n",
    "    all_evaluations['stacking'] = stacking_evaluation\n",
    "    \n",
    "    # Create comparison plot for all models\n",
    "    metrics = ['RMSE', 'MAE', 'R2']\n",
    "    for metric in metrics:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        model_names = list(all_evaluations.keys())\n",
    "        metric_values = [all_evaluations[model][metric] for model in model_names]\n",
    "        \n",
    "        # Create bar plot\n",
    "        sns.barplot(x=model_names, y=metric_values)\n",
    "        plt.title(f'Comparison of Models - {metric}')\n",
    "        plt.ylabel(metric)\n",
    "        plt.xlabel('Model')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Add a training time comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    model_names = list(base_models.keys()) + ['stacking']\n",
    "    time_values = [cv_results[name]['training_time'] for name in base_models.keys()] + [total_time]\n",
    "    \n",
    "    # Create bar plot for training times\n",
    "    sns.barplot(x=model_names, y=time_values)\n",
    "    plt.title('Comparison of Model Training Times')\n",
    "    plt.ylabel('Training Time (seconds)')\n",
    "    plt.xlabel('Model')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return stacking_regressor, stacking_evaluation\n",
    "# Build stacking model\n",
    "stacking_model, stacking_evaluation = build_stacking_model(trained_models, X_train, y_train, X_test, y_test)\n",
    "\n",
    "# 7. Save Models and Preprocessing Pipeline\n",
    "def save_pipeline(base_models, stacking_model, preprocessor, \n",
    "                 numerical_features, categorical_features, \n",
    "                 cv_results, evaluation, stacking_evaluation):\n",
    "    \"\"\"\n",
    "    Save all components of the machine learning pipeline\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    Various model components and metadata\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Dictionary of saved file paths\n",
    "    \"\"\"\n",
    "    print(\"\\nSaving machine learning pipeline components...\")\n",
    "    \n",
    "    saved_files = {}\n",
    "    \n",
    "    # Save base models\n",
    "    for name, model in base_models.items():\n",
    "        model_path = f'models/{name}_model.pkl'\n",
    "        joblib.dump(model, model_path)\n",
    "        saved_files[f'{name}_model'] = model_path\n",
    "    \n",
    "    # Save stacking model\n",
    "    stacking_path = 'models/stacking_model.pkl'\n",
    "    joblib.dump(stacking_model, stacking_path)\n",
    "    saved_files['stacking_model'] = stacking_path\n",
    "    \n",
    "    # Save preprocessor\n",
    "    preprocessor_path = 'models/preprocessor.pkl'\n",
    "    joblib.dump(preprocessor, preprocessor_path)\n",
    "    saved_files['preprocessor'] = preprocessor_path\n",
    "    \n",
    "    # Save feature lists\n",
    "    features = {\n",
    "        'numerical_features': numerical_features,\n",
    "        'categorical_features': categorical_features\n",
    "    }\n",
    "    features_path = 'models/feature_lists.pkl'\n",
    "    with open(features_path, 'wb') as f:\n",
    "        pickle.dump(features, f)\n",
    "    saved_files['features'] = features_path\n",
    "    \n",
    "    # Save evaluation results\n",
    "    results = {\n",
    "        'cv_results': cv_results,\n",
    "        'evaluation': evaluation,\n",
    "        'stacking_evaluation': stacking_evaluation\n",
    "    }\n",
    "    results_path = 'models/evaluation_results.pkl'\n",
    "    with open(results_path, 'wb') as f:\n",
    "        pickle.dump(results, f)\n",
    "    saved_files['results'] = results_path\n",
    "    \n",
    "    # Save model metadata\n",
    "    metadata = {\n",
    "        'creation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'model_names': list(base_models.keys()),\n",
    "        'feature_count': len(numerical_features) + len(categorical_features),\n",
    "        'numerical_features': numerical_features,\n",
    "        'categorical_features': categorical_features,\n",
    "        'target': 'log_price'\n",
    "    }\n",
    "    metadata_path = 'models/model_metadata.pkl'\n",
    "    with open(metadata_path, 'wb') as f:\n",
    "        pickle.dump(metadata, f)\n",
    "    saved_files['metadata'] = metadata_path\n",
    "    \n",
    "    print(\"Pipeline components saved successfully!\")\n",
    "    return saved_files\n",
    "\n",
    "# Save pipeline\n",
    "saved_files = save_pipeline(\n",
    "    trained_models, \n",
    "    stacking_model, \n",
    "    preprocessor, \n",
    "    numerical_features, \n",
    "    categorical_features, \n",
    "    cv_results, \n",
    "    evaluation, \n",
    "    stacking_evaluation\n",
    ")\n",
    "\n",
    "# 8. Load Models and Make Predictions\n",
    "def load_pipeline():\n",
    "    \"\"\"\n",
    "    Load all components of the saved machine learning pipeline\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Dictionary of loaded pipeline components\n",
    "    \"\"\"\n",
    "    print(\"\\nLoading machine learning pipeline components...\")\n",
    "    \n",
    "    pipeline = {}\n",
    "    \n",
    "    # Load metadata to get model names\n",
    "    with open('models/model_metadata.pkl', 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    pipeline['metadata'] = metadata\n",
    "    \n",
    "    # Load base models\n",
    "    base_models = {}\n",
    "    for name in metadata['model_names']:\n",
    "        model_path = f'models/{name}_model.pkl'\n",
    "        base_models[name] = joblib.load(model_path)\n",
    "    pipeline['base_models'] = base_models\n",
    "    \n",
    "    # Load stacking model\n",
    "    pipeline['stacking_model'] = joblib.load('models/stacking_model.pkl')\n",
    "    \n",
    "    # Load preprocessor\n",
    "    pipeline['preprocessor'] = joblib.load('models/preprocessor.pkl')\n",
    "    \n",
    "    # Load feature lists\n",
    "    with open('models/feature_lists.pkl', 'rb') as f:\n",
    "        features = pickle.load(f)\n",
    "    pipeline['numerical_features'] = features['numerical_features']\n",
    "    pipeline['categorical_features'] = features['categorical_features']\n",
    "    \n",
    "    # Load evaluation results\n",
    "    with open('models/evaluation_results.pkl', 'rb') as f:\n",
    "        results = pickle.load(f)\n",
    "    pipeline['cv_results'] = results['cv_results']\n",
    "    pipeline['evaluation'] = results['evaluation']\n",
    "    pipeline['stacking_evaluation'] = results['stacking_evaluation']\n",
    "    \n",
    "    print(\"Pipeline components loaded successfully!\")\n",
    "    return pipeline\n",
    "\n",
    "def make_predictions(df, pipeline, return_all_predictions=False):\n",
    "    \"\"\"\n",
    "    Make predictions using the loaded pipeline\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        New data for prediction\n",
    "    pipeline : dict\n",
    "        Loaded pipeline components\n",
    "    return_all_predictions : bool\n",
    "        Whether to return predictions from all models or just the stacking model\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with predictions\n",
    "    \"\"\"\n",
    "    print(\"\\nPreparing data for prediction...\")\n",
    "    \n",
    "    # Ensure we have all required features\n",
    "    numerical_features = pipeline['numerical_features']\n",
    "    categorical_features = pipeline['categorical_features']\n",
    "    \n",
    "    # Modified timestamp conversion in preprocess_data function\n",
    "    if 'timestamp' in df.columns:\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "        df['month'] = df['timestamp'].dt.month\n",
    "        df['day'] = df['timestamp'].dt.day\n",
    "        df['dayofweek'] = df['timestamp'].dt.dayofweek\n",
    "    \n",
    "    # Create text_length if text exists and text_length doesn't\n",
    "    if 'text' in df.columns and 'text_length' not in df.columns:\n",
    "        df['text_length'] = df['text'].fillna('').apply(len)\n",
    "    \n",
    "    # Select required features\n",
    "    X = df[numerical_features + categorical_features].copy()\n",
    "    \n",
    "    print(\"Making predictions...\")\n",
    "    \n",
    "    # Make predictions with each base model\n",
    "    predictions = {}\n",
    "    for name, model in pipeline['base_models'].items():\n",
    "        predictions[f'{name}_prediction'] = model.predict(X)\n",
    "    \n",
    "    # Make predictions with stacking model\n",
    "    predictions['stacking_prediction'] = pipeline['stacking_model'].predict(X)\n",
    "    \n",
    "    # Add predictions to the original dataframe\n",
    "    results_df = df.copy()\n",
    "    for name, preds in predictions.items():\n",
    "        results_df[name] = preds\n",
    "    \n",
    "    # Convert log_price predictions back to price if requested\n",
    "    if 'log_price' in results_df.columns:\n",
    "        for name in predictions.keys():\n",
    "            orig_price_col = name.replace('prediction', 'orig_price')\n",
    "            results_df[orig_price_col] = np.exp(results_df[name])\n",
    "    \n",
    "    print(\"Predictions completed!\")\n",
    "    \n",
    "    if return_all_predictions:\n",
    "        return results_df\n",
    "    else:\n",
    "        # Return only the stacking model predictions\n",
    "        selected_cols = list(df.columns) + ['stacking_prediction']\n",
    "        if 'log_price' in results_df.columns:\n",
    "            selected_cols.append('stacking_orig_price')\n",
    "        return results_df[selected_cols]\n",
    "\n",
    "# 9. Implement RAG (Retrieval-Augmented Generation) System\n",
    "def create_knowledge_base(df):\n",
    "    \"\"\"\n",
    "    Create a knowledge base from the dataset for retrieval\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pandas DataFrame\n",
    "        Dataset with text content\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    TF-IDF vectorizer and document vectors\n",
    "    \"\"\"\n",
    "    print(\"\\nCreating knowledge base from dataset...\")\n",
    "    \n",
    "    # Prepare text content for knowledge base\n",
    "    # Combine relevant columns into a single text field\n",
    "    if 'text' in df.columns:\n",
    "        # Create a combined text field with metadata\n",
    "        df['kb_text'] = df.apply(\n",
    "            lambda row: f\"Product: {row.get('asin', 'Unknown')} | \"\n",
    "                      + f\"Category: {row.get('main_category', 'Unknown')} | \"\n",
    "                      + f\"Rating: {row.get('rating', 'Unknown')} | \"\n",
    "                      + f\"Price: {np.exp(row['log_price']) if 'log_price' in df.columns else 'Unknown'} | \"\n",
    "                      + f\"Sentiment: {row.get('sentiment_category', 'Unknown')} | \"\n",
    "                      + f\"Text: {row.get('text', '')}\",\n",
    "            axis=1\n",
    "        )\n",
    "    else:\n",
    "        # If no text column, create metadata-only knowledge base\n",
    "        df['kb_text'] = df.apply(\n",
    "            lambda row: f\"Product: {row.get('asin', 'Unknown')} | \"\n",
    "                      + f\"Category: {row.get('main_category', 'Unknown')} | \"\n",
    "                      + f\"Rating: {row.get('rating', 'Unknown')} | \"\n",
    "                      + f\"Price: {np.exp(row['log_price']) if 'log_price' in df.columns else 'Unknown'} | \"\n",
    "                      + f\"Sentiment: {row.get('sentiment_category', 'Unknown')}\",\n",
    "            axis=1\n",
    "        )\n",
    "    \n",
    "    # Create TF-IDF vectorizer\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=5000, stop_words='english')\n",
    "    \n",
    "    # Create document vectors\n",
    "    doc_vectors = tfidf_vectorizer.fit_transform(df['kb_text'])\n",
    "    \n",
    "    print(f\"Knowledge base created with {doc_vectors.shape[0]} documents and {doc_vectors.shape[1]} features\")\n",
    "    \n",
    "    return tfidf_vectorizer, doc_vectors, df['kb_text'].tolist()\n",
    "\n",
    "def retrieve_relevant_documents(query, vectorizer, doc_vectors, documents, top_n=5):\n",
    "    \"\"\"\n",
    "    Retrieve relevant documents based on the query\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    query : str\n",
    "        User query\n",
    "    vectorizer : TfidfVectorizer\n",
    "        Fitted vectorizer\n",
    "    doc_vectors : sparse matrix\n",
    "        Document vectors\n",
    "    documents : list\n",
    "        Original documents\n",
    "    top_n : int\n",
    "        Number of documents to retrieve\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    List of relevant documents\n",
    "    \"\"\"\n",
    "    # Vectorize the query\n",
    "    query_vector = vectorizer.transform([query])\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarities = cosine_similarity(query_vector, doc_vectors).flatten()\n",
    "    \n",
    "    # Get indices of top similar documents\n",
    "    top_indices = similarities.argsort()[-top_n:][::-1]\n",
    "    \n",
    "    # Get top documents and their similarity scores\n",
    "    top_docs = [(documents[i], similarities[i]) for i in top_indices]\n",
    "    \n",
    "    return top_docs\n",
    "\n",
    "# 10. Integration with Chatbot Functions\n",
    "def generate_prediction_response(query, df_predictions, knowledge_base):\n",
    "    \"\"\"\n",
    "    Generate a response based on the query and predictions\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    query : str\n",
    "        User query\n",
    "    df_predictions : pandas DataFrame\n",
    "        DataFrame with predictions\n",
    "    knowledge_base : tuple\n",
    "        Knowledge base components (vectorizer, doc_vectors, documents)\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    Response text to be used by the chatbot\n",
    "    \"\"\"\n",
    "    vectorizer, doc_vectors, documents = knowledge_base\n",
    "    \n",
    "    # Retrieve relevant documents\n",
    "    relevant_docs = retrieve_relevant_documents(query, vectorizer, doc_vectors, documents)\n",
    "    \n",
    "    # Generate context from relevant documents\n",
    "    context = \"\\n\\n\".join([f\"Document (similarity: {score:.2f}):\\n{doc}\" \n",
    "                          for doc, score in relevant_docs])\n",
    "    \n",
    "    # Extract prediction stats\n",
    "    avg_price = np.exp(df_predictions['stacking_prediction'].mean())\n",
    "    min_price = np.exp(df_predictions['stacking_prediction'].min())\n",
    "    max_price = np.exp(df_predictions['stacking_prediction'].max())\n",
    "    \n",
    "    prediction_stats = f\"\"\"\n",
    "    Prediction Statistics:\n",
    "    - Average predicted price: ${avg_price:.2f}\n",
    "    - Minimum predicted price: ${min_price:.2f}\n",
    "    - Maximum predicted price: ${max_price:.2f}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Combine context and prediction stats to form the response\n",
    "    response = f\"\"\"\n",
    "    Based on your query: \"{query}\"\n",
    "    \n",
    "    {prediction_stats}\n",
    "    \n",
    "    Relevant information from the knowledge base:\n",
    "    {context}\n",
    "    \"\"\"\n",
    "    \n",
    "    return response\n",
    "\n",
    "def integrate_with_chatbot(df_predictions):\n",
    "    \"\"\"\n",
    "    Integrate the prediction system with existing chatbot functions\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df_predictions : pandas DataFrame\n",
    "        DataFrame with predictions\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    None - this function calls the chatbot functions\n",
    "    \"\"\"\n",
    "    print(\"\\nIntegrating prediction system with chatbot...\")\n",
    "    \n",
    "    # Create knowledge base\n",
    "    knowledge_base = create_knowledge_base(df_predictions)\n",
    "    \n",
    "    # Define a wrapper function to be used by the chatbot\n",
    "    def prediction_chatbot_handler(query):\n",
    "        # Generate response based on the query and predictions\n",
    "        response = generate_prediction_response(query, df_predictions, knowledge_base)\n",
    "        return response\n",
    "    \n",
    "    # Set up the prediction response generator\n",
    "    print(\"Setting up prediction response generator...\")\n",
    "    # This would be integrated with your existing generate_chat_response_v2 function\n",
    "    \n",
    "    # Start the audio chatbot\n",
    "    print(\"\\nStarting audio chatbot interaction...\")\n",
    "    print(\"Use '1' to start and '2' to stop the chatbot.\")\n",
    "    # This would call your existing run_audio_chatbot_v2 function\n",
    "    \n",
    "    # Note: The actual function calls are commented out since these are external functions\n",
    "    # In real implementation, you would uncomment these lines\n",
    "    # run_audio_chatbot_v2(start_trigger='1', stop_trigger='2', response_generator=prediction_chatbot_handler)\n",
    "    \n",
    "    print(\"\\nChatbot integration complete!\")\n",
    "\n",
    "# 11. Main Execution Function\n",
    "def run_prediction_pipeline(data_path='data_set.csv', retrain=False):\n",
    "    \"\"\"\n",
    "    Run the complete prediction pipeline\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data_path : str\n",
    "        Path to the input CSV file\n",
    "    retrain : bool\n",
    "        Whether to retrain the models or load existing ones\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with predictions\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    df = load_data(data_path)\n",
    "    \n",
    "    if retrain:\n",
    "        # Preprocess data\n",
    "        X_train, X_test, y_train, y_test, preprocessor, numerical_features, categorical_features = preprocess_data(df)\n",
    "        \n",
    "        # Train base models\n",
    "        trained_models, cv_results = train_base_models(X_train, y_train, preprocessor)\n",
    "        \n",
    "        # Evaluate base models\n",
    "        evaluation = evaluate_models(trained_models, X_test, y_test)\n",
    "        \n",
    "        # Build stacking model\n",
    "        stacking_model, stacking_evaluation = build_stacking_model(\n",
    "            trained_models, X_train, y_train, X_test, y_test\n",
    "        )\n",
    "        \n",
    "        # Save pipeline\n",
    "        saved_files = save_pipeline(\n",
    "            trained_models, \n",
    "            stacking_model, \n",
    "            preprocessor, \n",
    "            numerical_features, \n",
    "            categorical_features, \n",
    "            cv_results, \n",
    "            evaluation, \n",
    "            stacking_evaluation\n",
    "        )\n",
    "        \n",
    "        # Make predictions\n",
    "        pipeline = {\n",
    "            'base_models': trained_models,\n",
    "            'stacking_model': stacking_model,\n",
    "            'preprocessor': preprocessor,\n",
    "            'numerical_features': numerical_features,\n",
    "            'categorical_features': categorical_features\n",
    "        }\n",
    "        predictions_df = make_predictions(df, pipeline, return_all_predictions=True)\n",
    "        \n",
    "    else:\n",
    "        # Load existing pipeline\n",
    "        pipeline = load_pipeline()\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions_df = make_predictions(df, pipeline, return_all_predictions=True)\n",
    "    \n",
    "    # Integrate with chatbot\n",
    "    integrate_with_chatbot(predictions_df)\n",
    "    \n",
    "    return predictions_df\n",
    "\n",
    "# Run the complete pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    predictions_df = run_prediction_pipeline(retrain=True)\n",
    "    print(\"\\nPipeline execution complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e577e1df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
